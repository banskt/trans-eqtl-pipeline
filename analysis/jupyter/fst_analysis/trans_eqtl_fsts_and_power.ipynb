{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../../\")\n",
    "import collections\n",
    "import re\n",
    "import numpy as np\n",
    "from utils.readvcf_snp import ReadVCF\n",
    "\n",
    "pheno_file = \"/cbscratch/franco/datasets/gtex_v8/phenotypes/gtex_v8_basic_phenotypes.txt\"\n",
    "#admix_file = \"/cbscratch/franco/datasets/gtex_v8/genotypes/gtex-admixed0.9.txt\"\n",
    "gteur_file = \"/cbscratch/franco/datasets/gtex_v8/genotypes/gtex_v8_eur.sample\"\n",
    "gtall_file = \"/cbscratch/franco/datasets/gtex_v8/genotypes/gtex_v8.sample\"\n",
    "\n",
    "def read_samples(samplefile):\n",
    "    if os.path.exists(samplefile):\n",
    "        with open(samplefile, 'r') as samfile:\n",
    "            sample = 0\n",
    "            samplenames = list()\n",
    "            next(samfile)\n",
    "            next(samfile)\n",
    "            for line in samfile:\n",
    "                if re.search('^#', line):\n",
    "                    continue\n",
    "                sample += 1\n",
    "                samplenames.append(line.strip().split()[0])\n",
    "        nsample = sample\n",
    "        samplenames = samplenames\n",
    "        return samplenames, nsample\n",
    "    \n",
    "allsamples, nall = read_samples(gtall_file)\n",
    "\n",
    "sample_pheno_dict = dict()\n",
    "with open(pheno_file) as instream:\n",
    "    for line in instream:\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "        if re.search(\"#\", line):\n",
    "            continue\n",
    "        if re.search(\"dbGaP\", line):\n",
    "            header = line.strip().split()\n",
    "            continue\n",
    "        arr = line.strip().split(\"\\t\")\n",
    "        sampleid = arr[1]\n",
    "        race = arr[5]\n",
    "        sample_pheno_dict[sampleid] = race\n",
    "        \n",
    "# \"1\" \"Asian\"\n",
    "# \"2\" \"Black or African American\"\n",
    "# \"3\" \"White\"\n",
    "# \"4\" \"American Indian or Alaska Native\"\n",
    "# \"98\" \"Not Reported\"\n",
    "# \"99\" \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_simple_fst_power(pops_gt, pops): \n",
    "    \n",
    "    nsnps = pops_gt[pops[0]].shape[0]\n",
    "    for j in range(1, len(pops)):\n",
    "        if nsnps != pops_gt[pops[j]].shape[0]:\n",
    "            print(\"SNP numbers differ between populations\")\n",
    "            raise\n",
    "    \n",
    "    power_list = list()\n",
    "    fst_list = list()\n",
    "    # all_gt = np.hstack((eur_gt1kg, afr_gt1kg))\n",
    "    all_gt = np.hstack(tuple([pops_gt[x] for x in pops]))\n",
    "    all_n  = all_gt.shape[1] # size of total population\n",
    "    for snpi in range(all_gt.shape[0]):\n",
    "        #print(snp_info[snpi])\n",
    "        # maf_all = sum(all_gt[snpi,:] / 2 / len(all_gt[snpi,:]))\n",
    "        # maf_eur = sum(eur_gt[snpi,:] / 2 / len(eur_gt[snpi,:]))\n",
    "        # maf_afr = sum(afr_gt[snpi,:] / 2 / len(afr_gt[snpi,:]))\n",
    "        maf_all = sum(all_gt[snpi,:] / 2 / len(all_gt[snpi,:]))\n",
    "        maf_pops = [sum(pops_gt[x][snpi,:] / 2 / len(pops_gt[x][snpi,:])) for x in pops]\n",
    "\n",
    "        c_pops = [pops_gt[x].shape[1]/all_n for x in pops]\n",
    "        #c_eur = eur_n / all_n\n",
    "        #c_afr = afr_n / all_n\n",
    "        \n",
    "        p_all = maf_all*(1-maf_all)\n",
    "        sum_p = np.sum(np.array([c_pops[i]*maf_pops[i]*(1-maf_pops[i]) for i in range(len(pops))]))\n",
    "        power = all_n * sum_p # (c_eur*maf_eur*(1-maf_eur) + c_afr*maf_afr*(1-maf_afr) )\n",
    "        \n",
    "        Fst = (p_all  - sum_p ) / p_all\n",
    "        power_list.append(power)\n",
    "        fst_list.append(Fst)\n",
    "    return fst_list, power_list\n",
    "\n",
    "\n",
    "def get_gt_count(dosage, allele='ref'):\n",
    "    n_het = dosage.count(1)\n",
    "    if allele == \"ref\":\n",
    "        n_hom = dosage.count(0)\n",
    "    if allele == \"alt\":\n",
    "        n_hom = dosage.count(2)\n",
    "    return n_hom, n_het\n",
    "\n",
    "def weir_cockerman_fst(pops_gt, pops=[\"eur\", \"afr\"], alleles=[\"ref\", \"alt\"]):\n",
    "    \n",
    "    fst_list = list()\n",
    "    nsnps = pops_gt[pops[0]].shape[0]\n",
    "    if nsnps != pops_gt[pops[1]].shape[0]:\n",
    "        print(\"SNP numbers differ between populations\")\n",
    "        raise\n",
    "        \n",
    "    for snpi in range(nsnps):\n",
    "        n_pops = len(pops)\n",
    "        n_alleles = len(alleles)\n",
    "        n = np.zeros(n_pops)\n",
    "        p = np.zeros((n_pops, n_alleles))\n",
    "        pbar = np.zeros(n_alleles)\n",
    "        hbar = np.zeros(n_alleles)\n",
    "        ssqr = np.zeros(n_alleles)\n",
    "\n",
    "        nbar = 0\n",
    "        sum_nsqr = 0\n",
    "        for pop in range(n_pops):\n",
    "            for al in range(n_alleles):\n",
    "                n_hom, n_het = get_gt_count(list(pops_gt[pops[pop]][snpi,:]), allele=alleles[al])\n",
    "                n[pop] += n_hom + 0.5*n_het\n",
    "                p[pop,al] = n_het + 2*n_hom\n",
    "\n",
    "                nbar += n[pop]\n",
    "                pbar[al] += p[pop][al]\n",
    "                hbar[al] += n_het\n",
    "            for al in range(n_pops):\n",
    "                p[pop,al] /= 2.0*n[pop]\n",
    "\n",
    "            sum_nsqr += (n[pop] * n[pop])\n",
    "\n",
    "        n_sum = sum(n)\n",
    "        nbar  = n_sum / n_pops\n",
    "\n",
    "        for al in range(n_alleles):\n",
    "            pbar[al] /= n_sum * 2.0\n",
    "            hbar[al] /= n_sum\n",
    "\n",
    "        for al in range(n_alleles):\n",
    "            for pop in range(n_pops):\n",
    "                ssqr[al] += n[pop]*(p[pop,al] - pbar[al])*(p[pop,al] - pbar[al])\n",
    "            ssqr[al] /= (n_pops-1)*nbar\n",
    "        nc = (n_sum - (sum_nsqr / n_sum)) / (n_pops - 1)\n",
    "\n",
    "        snp_Fst = np.zeros(n_alleles)\n",
    "        a = np.zeros(n_alleles)\n",
    "        b = np.zeros(n_alleles)\n",
    "        c = np.zeros(n_alleles)\n",
    "        r = n_pops\n",
    "        sum_a = 0\n",
    "        sum_all = 0\n",
    "        for al in range(n_alleles):\n",
    "            a[al] = (ssqr[al] - ( pbar[al]*(1.0-pbar[al]) - (((r-1.0)*ssqr[al])/r) - (hbar[al]/4.0) )/(nbar-1.0))*nbar/nc;\n",
    "            b[al] = (pbar[al]*(1.0-pbar[al]) - (ssqr[al]*(r-1.0)/r) - hbar[al]*( ((2.0*nbar)-1.0) / (4.0*nbar) ))*nbar / (nbar-1.0) ;\n",
    "            c[al] = hbar[al] / 2.0;\n",
    "            snp_Fst[al] = a[al]/(a[al]+b[al]+c[al]);\n",
    "\n",
    "            if not np.all([np.isnan(a[al]),np.isnan(b[al]),np.isnan(c[al])]):\n",
    "                sum_a += a[al]\n",
    "                sum_all += a[al] + b[al] + c[al]\n",
    "        fst = sum_a/sum_all\n",
    "        fst_list.append(fst)\n",
    "    return fst_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import utils\n",
    "\n",
    "tissue_file = \"../../plots/tissue_table.txt\"\n",
    "json_file   = \"../../gtex_v8_metadata.json\"\n",
    "tshorts, tfulls, tstrings = utils.read_tissues_str(tissue_file)\n",
    "with open(json_file) as instream:\n",
    "    gtex_meta = json.load(instream)\n",
    "tissue_colors = dict()\n",
    "tissue_names = dict()\n",
    "tissue_nsamples = dict()\n",
    "\n",
    "for tshort, tfull, tstring in zip(tshorts, tfulls, tstrings):\n",
    "    if tshort in tshorts:\n",
    "        tissue_names[tshort] = tstring\n",
    "        tissue_colors[tshort] = \"#\" + gtex_meta[tfull][\"colorHex\"]\n",
    "        tissue_nsamples[tshort] = gtex_meta[tfull][\"rnaSeqSampleCount\"]\n",
    "        \n",
    "brain_tissues = ['bam', 'ban', 'bca', 'bceh', 'bce', 'bco', 'bfr', 'bhi', 'bhy', 'bnu', 'bpu', 'bsp', 'bsu']\n",
    "altsb_tissues = ['haa', 'pan', 'spl', 'wb']\n",
    "\n",
    "SNPRES_FIELDS = ['rsid', 'chrom', 'pos', 'logp', 'maf']\n",
    "class SNPRes(collections.namedtuple('_SNPRes', SNPRES_FIELDS)):\n",
    "    __slots__ = ()\n",
    "\n",
    "def tejaas(filepath):\n",
    "    res = list()\n",
    "    with open(filepath, 'r') as mfile:\n",
    "        next(mfile)\n",
    "        for line in mfile:\n",
    "            arr   = line.strip().split(\"\\t\")\n",
    "            rsid  = arr[0]\n",
    "            #chrom = rsid.split(\"_\")[0][3:]\n",
    "            chrom = int(arr[1])\n",
    "            pos   = int(arr[2])\n",
    "            p     = float(arr[7])\n",
    "            logp  = np.log10(p) if p!=0 else np.log10(10e-30)\n",
    "            maf   = float(arr[3])\n",
    "            res.append(SNPRes(rsid=rsid, chrom=chrom, pos=pos, logp=-logp, maf=maf))\n",
    "    return res    \n",
    "    \n",
    "basename = \"protein_coding_lncRNA_{:s}_knn30_cut5e-8\"\n",
    "gammas = [\"gamma01\", \"gamma0006\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  as has 586 trans-eqtls\n",
      "Loading  av has 463 trans-eqtls\n",
      "Loading  ag has 184 trans-eqtls\n",
      "Loading  aa has 1298 trans-eqtls\n",
      "Loading  ac has 1539 trans-eqtls\n",
      "Loading  at has 391 trans-eqtls\n",
      "Loading  bam has 1182 trans-eqtls\n",
      "Loading  ban has 954 trans-eqtls\n",
      "Loading  bca has 19 trans-eqtls\n",
      "Loading  bceh has 128 trans-eqtls\n",
      "Loading  bce has 40 trans-eqtls\n",
      "Loading  bco has 64 trans-eqtls\n",
      "Loading  bfr has 32 trans-eqtls\n",
      "Loading  bhi has 51 trans-eqtls\n",
      "Loading  bhy has 83 trans-eqtls\n",
      "Loading  bnu has 379 trans-eqtls\n",
      "Loading  bpu has 10 trans-eqtls\n",
      "Loading  bsp has 61 trans-eqtls\n",
      "Loading  bsu has 28 trans-eqtls\n",
      "Loading  br has 505 trans-eqtls\n",
      "Loading  ebv has 297 trans-eqtls\n",
      "Loading  fib has 209 trans-eqtls\n",
      "Loading  cols has 75 trans-eqtls\n",
      "Loading  colt has 1174 trans-eqtls\n",
      "Loading  esog has 17 trans-eqtls\n",
      "Loading  esom has 9 trans-eqtls\n",
      "Loading  esomu has 20 trans-eqtls\n",
      "Loading  haa has 64 trans-eqtls\n",
      "Loading  hlv has 247 trans-eqtls\n",
      "Loading  kc has 14 trans-eqtls\n",
      "Loading  liv has 37 trans-eqtls\n",
      "Loading  lu has 477 trans-eqtls\n",
      "Loading  msg has 3 trans-eqtls\n",
      "Loading  ms has 2253 trans-eqtls\n",
      "Loading  nt has 720 trans-eqtls\n",
      "Loading  ov has 80 trans-eqtls\n",
      "Loading  pan has 179 trans-eqtls\n",
      "Loading  pit has 10 trans-eqtls\n",
      "Loading  pro has 886 trans-eqtls\n",
      "Loading  snse has 725 trans-eqtls\n",
      "Loading  sse has 400 trans-eqtls\n",
      "Loading  si has 82 trans-eqtls\n",
      "Loading  spl has 1354 trans-eqtls\n",
      "Loading  sto has 252 trans-eqtls\n",
      "Loading  tes has 1286 trans-eqtls\n",
      "Loading  thy has 998 trans-eqtls\n",
      "Loading  ut has 1683 trans-eqtls\n",
      "Loading  va has 0 trans-eqtls\n",
      "Loading  wb has 6269 trans-eqtls\n"
     ]
    }
   ],
   "source": [
    "basepath = \"/cbscratch/franco/trans-eqtl\"\n",
    "trans_dict = dict()\n",
    "for tissue in tshorts:\n",
    "    if tissue in altsb_tissues:\n",
    "        config = basename.format(gammas[1])\n",
    "    else:\n",
    "        config = basename.format(gammas[0])\n",
    "    tejaas_file = os.path.join(basepath, config, tissue, \"trans_eqtls_ldpruned.txt\")\n",
    "    \n",
    "    if not os.path.exists(tejaas_file):\n",
    "        print(\"{:s} has no trans-eqtl results\".format(tissue))\n",
    "        continue\n",
    "    print(\"Loading \", tissue, end=\"\")\n",
    "    transeqtls = tejaas(tejaas_file)\n",
    "    if len(transeqtls) > 0:\n",
    "        trans_dict[tissue] = transeqtls\n",
    "        print(\" has {:d} trans-eqtls\".format(len(transeqtls)))\n",
    "    else:\n",
    "        trans_dict[tissue] = []\n",
    "        print(\" has 0 trans-eqtls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1 has 1423 trans-eqtls\n",
      "chr2 has 1629 trans-eqtls\n",
      "chr3 has 1410 trans-eqtls\n",
      "chr4 has 1180 trans-eqtls\n",
      "chr5 has 1003 trans-eqtls\n",
      "chr6 has 1176 trans-eqtls\n",
      "chr7 has 1075 trans-eqtls\n",
      "chr8 has 855 trans-eqtls\n",
      "chr9 has 906 trans-eqtls\n",
      "chr10 has 983 trans-eqtls\n",
      "chr11 has 853 trans-eqtls\n",
      "chr12 has 827 trans-eqtls\n",
      "chr13 has 495 trans-eqtls\n",
      "chr14 has 637 trans-eqtls\n",
      "chr15 has 722 trans-eqtls\n",
      "chr16 has 915 trans-eqtls\n",
      "chr17 has 720 trans-eqtls\n",
      "chr18 has 529 trans-eqtls\n",
      "chr19 has 414 trans-eqtls\n",
      "chr20 has 514 trans-eqtls\n",
      "chr21 has 301 trans-eqtls\n",
      "chr22 has 284 trans-eqtls\n",
      "18851\n",
      "18851\n"
     ]
    }
   ],
   "source": [
    "teqtl_varids = list()\n",
    "for tissue in tshorts:\n",
    "    teqtl_varids += [snp.rsid for snp in trans_dict[tissue]]\n",
    "teqtl_varids = list(set(teqtl_varids))\n",
    "\n",
    "chrm_teqtls = dict()\n",
    "suma = 0\n",
    "for chrm in range(1,23):\n",
    "    chrm_teqtls[chrm] = [x for x in teqtl_varids if x.startswith(\"chr{:d}_\".format(chrm))]\n",
    "    print(f\"chr{chrm} has {len(chrm_teqtls[chrm])} trans-eqtls\")\n",
    "    suma += len(chrm_teqtls[chrm])\n",
    "print(suma)\n",
    "print(len(teqtl_varids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CHR1\n",
      "all 1423 found!\n",
      "Reading CHR2\n",
      "all 1629 found!\n",
      "Reading CHR3\n",
      "all 1410 found!\n",
      "Reading CHR4\n",
      "all 1180 found!\n",
      "Reading CHR5\n",
      "all 1003 found!\n",
      "Reading CHR6\n",
      "all 1176 found!\n",
      "Reading CHR7\n",
      "all 1075 found!\n",
      "Reading CHR8\n",
      "all 855 found!\n",
      "Reading CHR9\n",
      "all 906 found!\n",
      "Reading CHR10\n",
      "all 983 found!\n",
      "Reading CHR11\n",
      "all 853 found!\n",
      "Reading CHR12\n",
      "all 827 found!\n",
      "Reading CHR13\n",
      "all 495 found!\n",
      "Reading CHR14\n",
      "all 637 found!\n",
      "Reading CHR15\n",
      "all 722 found!\n",
      "Reading CHR16\n",
      "all 915 found!\n",
      "Reading CHR17\n",
      "all 720 found!\n",
      "Reading CHR18\n",
      "all 529 found!\n",
      "Reading CHR19\n",
      "all 414 found!\n",
      "Reading CHR20\n",
      "all 514 found!\n",
      "Reading CHR21\n",
      "all 301 found!\n",
      "Reading CHR22\n",
      "all 284 found!\n"
     ]
    }
   ],
   "source": [
    "SNPGT_FIELDS = ['varid', 'chrom', 'pos', 'maf', 'dosage']\n",
    "class SNPGT(collections.namedtuple('_SNPGT', SNPGT_FIELDS)):\n",
    "    __slots__ = ()\n",
    "\n",
    "full_teqtls_gt = collections.defaultdict(dict)\n",
    "first_donors = list()\n",
    "for chrm in range(1,23):\n",
    "    print(f\"Reading CHR{chrm}\")\n",
    "    f_vcf = \"/cbscratch/franco/datasets/gtex_v8/genotypes/vcfs_SHAPEIT2/0.01/GTEX_v8_2020-02-21_WGS_838Indiv_Freeze.SHAPEIT2_phased_NoMissingGT_SNPfilter_MAF0.01_chr{:d}.vcf.gz\".format(chrm)\n",
    "    # f_vcf = \"/cbscratch/franco/datasets/gtex_v8/genotypes/vcfs_SHAPEIT2/ldpruned/GTEX_v8.SHAPEIT2_chr1.ldpruned.vcf.gz\"\n",
    "    # samplefile = \"/cbscratch/franco/datasets/gtex_v8/genotypes/gtex_v8.sample\"\n",
    "    samplefile = None\n",
    "    vcf = ReadVCF(f_vcf, snplist=chrm_teqtls[chrm])\n",
    "    gtfull = vcf.dosage\n",
    "    gt_donors = vcf.donor_ids\n",
    "    if chrm == 1:\n",
    "        first_donors = gt_donors\n",
    "    else:\n",
    "        if first_donors != gt_donors or len(gt_donors) != len(first_donors):\n",
    "            print(\"donor error!\")\n",
    "            raise\n",
    "    snpinfos = vcf.snpinfo\n",
    "    for i,snp in enumerate(snpinfos):\n",
    "        full_teqtls_gt[chrm][snp.varid] = SNPGT(varid=snp.varid, chrom=snp.chrom, pos=snp.bp_pos, maf=snp.maf, dosage=gtfull[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the samples for each tissue\n",
    "expr_dir = \"/cbscratch/franco/trans-eqtl/new_preprocess_feb2020_freeze/gtex_v8/expression/tpms\"\n",
    "expr_file = os.path.join(expr_dir, \"{:s}_tpms_qcfilter.txt\")\n",
    "\n",
    "tissue_samples = dict()\n",
    "for tissue in tshorts:\n",
    "    with open(expr_file.format(tissue)) as instream:\n",
    "        samplenames = instream.readline().strip().split(\"\\t\")[1:]\n",
    "        tissue_samples[tissue] = samplenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n",
      "chr1_1170732_A_G_b38\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-dcd5b1de809a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mfsts_gtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_gtex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_simple_fst_power\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpops_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mweir_fsts_gtex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweir_cockerman_fst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpops_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malleles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "def match_samples(gt_donors, expr_donors):\n",
    "    common  = [x for x in gt_donors if x in expr_donors]\n",
    "    vcfmask = [gt_donors.index(x) for x in common]\n",
    "    return vcfmask, common\n",
    "\n",
    "def find_ancestry(samples, sample_pheno_dict):\n",
    "    ix_eur = list()\n",
    "    ix_afr = list()\n",
    "    for i,sid in enumerate(samples):\n",
    "        if sample_pheno_dict[sid] == '2':\n",
    "            ix_afr.append(i)\n",
    "        if sample_pheno_dict[sid] == '3':\n",
    "            ix_eur.append(i)\n",
    "    return ix_eur, ix_afr\n",
    "\n",
    "for tissue in tshorts:\n",
    "tissue = \"as\"\n",
    "vcfmask, sampids = match_samples(gt_donors, tissue_samples[tissue])\n",
    "pops = [\"eur\", \"afr\"]\n",
    "alleles = [\"ref\", \"alt\"]\n",
    "ix_eur, ix_afr = find_ancestry(sampids, sample_pheno_dict)\n",
    "print(len(vcfmask))\n",
    "for snp in trans_dict[tissue]:\n",
    "    print(snp.rsid)\n",
    "    chrm = int(snp.rsid.split(\"_\")[0][3:])\n",
    "    snp_data = full_teqtls_gt[chrm][snp.rsid]\n",
    "    pops_gt = dict()\n",
    "    pops_gt[\"eur\"] = np.array([snp_data.dosage[i] for i in ix_eur]).reshape(1,-1)\n",
    "    pops_gt[\"afr\"] = np.array([snp_data.dosage[i] for i in ix_afr]).reshape(1,-1)\n",
    "    fsts_gtex, power_gtex = calc_simple_fst_power(pops_gt, pops)\n",
    "    weir_fsts_gtex = weir_cockerman_fst(pops_gt, pops, alleles)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.003201556332503024] [49.06665092179091] [0.0077605747023750836]\n"
     ]
    }
   ],
   "source": [
    "print(fsts_gtex, power_gtex, weir_fsts_gtex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
